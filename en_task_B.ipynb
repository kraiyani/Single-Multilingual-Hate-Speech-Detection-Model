{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import copy\n",
    "import re\n",
    "import csv\n",
    "from keras.utils import to_categorical\n",
    "import codecs\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation , LSTM , Input , Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, concatenate, Activation, Average\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout , Bidirectional\n",
    "from keras.layers import Flatten , LSTM , Reshape, LeakyReLU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.regularizers import L1L2\n",
    "from keras import optimizers\n",
    "from keras.callbacks import CSVLogger\n",
    "import json\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'id', u'text', u'HS', u'TR', u'AG']\n"
     ]
    }
   ],
   "source": [
    "#train.tsv\n",
    "train=pd.read_csv('train_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "print(list(train.columns.values)) #file header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id      10000\n",
       "text    10000\n",
       "HS      10000\n",
       "TR      10000\n",
       "AG      10000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Hurray, saving us $$$ in so many ways @potus @...\n",
      "1    ;Why would young fighting age men be the vast ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(my_df.text.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_word_stop = ['the','in','of','is','a','to','an','be','are','for','was','it','as','on', 'so', 'at']\n",
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_data(data):\n",
    "#     tokenized_data = []\n",
    "#     for line in data:\n",
    "# #         for i in line.split():\n",
    "# #             if i[:1] == '#':\n",
    "# #                 line = line.strip() + ' ' + i[1:]\n",
    "# #             elif i[:1] == '@':\n",
    "# #                 line = line.strip() + ' ' + i[1:]\n",
    "#         line = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\",\"\",line)\n",
    "#         line = re.sub(r\"(.)\\1+\", r\"\\1\",line)\n",
    "#         #line = re.sub(r\"#\\S+\", \"\",line)\n",
    "#         #line = re.sub(r\"@\\S+\", \"\",line)\n",
    "#         #line = re.sub(r'([!.,?();*:\\[\\]\":\\”\\“])([\\w!.,?();*\\[\\]\":\\”\\“])', r'\\1 \\2',line)\n",
    "#         #line = \" \".join([word for word in line.split() if word not in my_word_stop])\n",
    "#         tokenized_data.append(line)\n",
    "#     return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls = []\n",
    "# Xs = []\n",
    "# ls = tokenize_data(my_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ls[0])\n",
    "# print(my_df.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('segmentation_train_dev.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in my_df.text:\n",
    "        row = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\",\"url\",row)\n",
    "        row = \" \".join([word for word in row.split() if word not in my_word_stop])\n",
    "        row = re.sub(r\"(.)\\1+\", r\"\\1\",row)\n",
    "        #\" \".join([word for word in line.split()\n",
    "        rest_array = [text.encode(\"utf8\") for text in row]\n",
    "        rest_array = \"\".join(rest_array)\n",
    "        #rest_array = rest_array.replace(',','')\n",
    "        writer.writerow(rest_array)\n",
    "        #print(rest_array)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(row)\n",
    "# print(rest_array[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('segmentation_train_dev.csv',delimiter='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0] = train[0].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = my_df.AG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df.HS.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Tokenizer\n",
    "tokenizer = Tokenizer(lower=False,filters=',',char_level=False)\n",
    "# feed our posts to the Tokenizer\n",
    "tokenizer.fit_on_texts(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "# Let's save this out so we can use it later\n",
    "with open('dictionary.p', 'wb') as fp:\n",
    "    pickle.dump(dictionary, fp)\n",
    "\n",
    "# with open('dictionary.json', 'w') as dictionary_file:\n",
    "#     json.dump(dictionary, dictionary_file, ensure_ascii=False)\n",
    "\n",
    "def convert_text_to_index_array(text):\n",
    "    # one really important thing that `text_to_word_sequence` does\n",
    "    # is make all texts the same length -- in this case, the length\n",
    "    # of the longest text in the set.\n",
    "    temp_wordIndices = []\n",
    "    for word in kpt.text_to_word_sequence(text,filters=',',lower=False):\n",
    "        if word in dictionary:\n",
    "            temp_wordIndices.append(dictionary[word])\n",
    "    return temp_wordIndices\n",
    "\n",
    "allWordIndices = []\n",
    "# for each post, change each token to its ID in the Tokenizer's word_index\n",
    "for text in train_x:\n",
    "    #print(text)\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "# now we have a list of all posts converted to index arrays.\n",
    "# cast as an array for future usage.\n",
    "allWordIndices = np.asarray(allWordIndices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 169 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s unique tokens.' % len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one-hot matrices out of the indexed posts\n",
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "# treat the labels as categories\n",
    "train_y = to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape[1]\n",
    "#train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 100)               17100     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 37,702\n",
      "Trainable params: 37,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.4507 - acc: 0.8210 - val_loss: 0.4242 - val_acc: 0.8405\n",
      "Epoch 2/2\n",
      "8000/8000 [==============================] - 8s 990us/step - loss: 0.4329 - acc: 0.8267 - val_loss: 0.4229 - val_acc: 0.8325\n",
      "saved model!\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(100, activation='relu',input_shape=(train_x.shape[1],)))\n",
    "#model.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1,input_shape=(train_x.shape[1],)))\n",
    "#model.add(GlobalMaxPooling1D())\n",
    "#model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "#model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "filepath=\"sequencing_the_data_try_n_error.{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "csv_logger = CSVLogger('final_log.csv', append=True, separator=';')\n",
    "\n",
    "model.fit(train_x, train_y,\n",
    "    batch_size=1,\n",
    "    epochs=2,\n",
    "    verbose=1,\n",
    "    validation_split=0.20,\n",
    "    shuffle=True,callbacks = [csv_logger])\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')\n",
    "\n",
    "print('saved model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'id', u'text']\n"
     ]
    }
   ],
   "source": [
    "#test.tsv\n",
    "test=pd.read_csv('test_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "print(list(test.columns.values)) #file header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('segmentation_test.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in test.text:\n",
    "        row = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\",\"url\",row)\n",
    "        row = \" \".join([word for word in row.split() if word not in my_word_stop])\n",
    "        row = re.sub(r\"(.)\\1+\", r\"\\1\",row)\n",
    "        #\" \".join([word for word in line.split()\n",
    "        rest_array = [text.encode(\"utf8\") for text in row]\n",
    "        rest_array = \"\".join(rest_array)\n",
    "        writer.writerow(rest_array)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "test_final=pd.read_csv('segmentation_test.csv',delimiter='\\t',header=None)\n",
    "print(list(test_final.columns.values)) #file header\n",
    "test_final[0] = test_final[0].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "('word not found : ', 0)\n"
     ]
    }
   ],
   "source": [
    "# we're still going to use a Tokenizer here, but we don't need to fit it\n",
    "tokenizer = Tokenizer(num_words=train_x.shape[1])\n",
    "# for human-friendly printing\n",
    "#labels = ['OAG','CAG','NAG']\n",
    "\n",
    "# read in our saved dictionary\n",
    "\n",
    "with open('dictionary.p', 'rb') as fp:\n",
    "    dictionary = pickle.load(fp)\n",
    "    \n",
    "    \n",
    "# with open('dictionary.json', 'r') as dictionary_file:\n",
    "#     dictionary = json.load(dictionary_file)\n",
    "\n",
    "# this utility makes sure that all the words in your input\n",
    "# are registered in the dictionary\n",
    "# before trying to turn them into a matrix.\n",
    "not_found_word_list = []\n",
    "def convert_text_to_index_array(text):\n",
    "    words = kpt.text_to_word_sequence(text,filters='',lower=False,split=',')\n",
    "    wordIndices = []\n",
    "    no_word = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "        else:\n",
    "            #print(\"'%s' not in training corpus; ignoring.\" %(word))\n",
    "            if word == \"\":\n",
    "                not_found_word_list.append(word)\n",
    "                no_word = no_word + 1\n",
    "    \n",
    "    return wordIndices,no_word\n",
    "\n",
    "# read in your saved model structure\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "# and create a model from that\n",
    "model = model_from_json(loaded_model_json)\n",
    "# and weight your nodes with your saved values\n",
    "model.load_weights('model.h5')\n",
    "with open('fileName.csv', 'w') as f:\n",
    "    count=0\n",
    "    no_words = 0\n",
    "    for row in test_final[0]:\n",
    "        # okay here's the interactive part\n",
    "        evalSentence = row\n",
    "        # format your input for the neural net\n",
    "        testArr,no_word = convert_text_to_index_array(evalSentence)\n",
    "        input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "        # predict which bucket your input belongs in\n",
    "        pred = model.predict(input)\n",
    "        # and print it for the humons\n",
    "        f.write(str(np.argmax(pred)) + \"\\n\")\n",
    "        #f.write(np.argmax(pred).astype('str') + \"\\n\")\n",
    "        #f.write(pred + \"\\n\")\n",
    "        count+=1\n",
    "        no_words+=no_word\n",
    "f.close()\n",
    "print(count)\n",
    "print(\"word not found : \", no_words)\n",
    "with open('not_found_word_list.csv', 'w') as f:\n",
    "    for word in not_found_word_list:\n",
    "        f.write(str(word)+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
